# -*- coding: utf-8 -*-
"""codepythonMERR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jYCopcy4Dc6RFq2gbGVI_pvlNDPm9Y5_
"""

import pandas as pd
import numpy as np
import math
import numpy as np
import ast
import matplotlib.pyplot as plt
import seaborn as sns

details = pd.read_csv('details.csv')
details['start_date'] = pd.to_datetime(details['start_date'], errors='coerce')
details = details.sort_values('start_date')
anime_works = pd.read_csv('person_anime_works.csv')
voice_works = pd.read_csv('person_voice_works.csv')
person_details = pd.read_csv('person_details.csv')
stats = pd.read_csv('stats.csv')
#ratings = pd.read_csv('ratings.csv')
aw_light = anime_works[['person_mal_id', 'anime_mal_id']]
vw_light = voice_works[['person_mal_id', 'anime_mal_id']]
all_staff = pd.concat([aw_light, vw_light]).drop_duplicates()

def calculate_variance(row):
    total = row['total']
    if total == 0: return 0
    mean_score = sum([row[f'score_{i}_votes'] * i for i in range(1, 11)]) / total
    variance = sum([row[f'score_{i}_votes'] * ((i - mean_score) ** 2) for i in range(1, 11)]) / total
    return variance

stats['controversy_score'] = (stats['score_1_votes'] + stats['score_10_votes']) / stats['total']
details = pd.merge(details, stats[['mal_id', 'controversy_score']], on='mal_id', how='left')
details['controversy_score'] = details['controversy_score'].fillna(0)

# 2. On merge avec les details pour avoir la DATE et le SCORE de chaque anime
# C'est ce qui nous permet de savoir "quand" le staff a travaillé et "quel succès" ils ont eu
staff_history = pd.merge(all_staff, details[['mal_id', 'start_date', 'score']],
                         left_on='anime_mal_id', right_on='mal_id')

# 3. On trie par date (Crucial pour ne pas voir le futur)
staff_history = staff_history.sort_values('start_date')

# 4. Calcul de la réputation PASSÉE (Expanding Window)
# Pour chaque personne, on calcule la moyenne des scores de ses animes PRÉCÉDENTS.
# shift(1) est vital : il exclut l'anime actuel du calcul (Pas de Data Leakage)
staff_history['person_past_performance'] = staff_history.groupby('person_mal_id')['score']\
    .transform(lambda x: x.shift(1).expanding().mean())

# Gestion des nouveaux venus (Cold Start Staff)
# Si un membre du staff n'a rien fait avant, on lui attribue la moyenne globale (ou une note neutre comme 6.5)
global_mean_score = details['score'].mean()
staff_history['person_past_performance'] = staff_history['person_past_performance'].fillna(global_mean_score)

# 5. Agrégation par Anime
# On calcule le score moyen du staff pour l'anime en question, basé uniquement sur leur passé.
anime_staff_score = staff_history.groupby('anime_mal_id').agg(
    avg_staff_track_record=('person_past_performance', 'mean'), # La Qualité "Prouvée"
    staff_count=('person_mal_id', 'count')                      # La Quantité
).reset_index()

# 6. Application de ta formule Logarithmique (que tu as validée par tes graphes)
# On combine la qualité (historique) et la quantité (taille du staff)
# Tu peux utiliser ton 'best_alpha' trouvé dans tes graphes ici s'il est fixe, sinon garde la séparation
anime_staff_score['staff_quality_log'] = np.log1p(anime_staff_score['avg_staff_track_record'])
anime_staff_score['staff_quantity_log'] = np.log1p(anime_staff_score['staff_count'])

# On merge ça dans details
details = pd.merge(details, anime_staff_score, left_on='mal_id', right_on='anime_mal_id', how='left')

# Remplir les animes sans staff connu (rares, mais possible)
details['staff_quality_log'] = details['staff_quality_log'].fillna(np.log1p(global_mean_score))
details['staff_quantity_log'] = details['staff_quantity_log'].fillna(0)

###################################################
####### drop/modif des colonnes inutiles ##########
###################################################

details = details.drop('url', axis=1) # Clairement inutile

details = details.drop('explicit_genres', axis=1) # J'ai aucune idée de ce que c'est, alors dans le doute...

details['start_date'] = pd.to_datetime(details['start_date'], errors='coerce') # On utilise start_date car elle a beaucoup moins de NA que year/season
details = details.dropna(subset=['start_date'])

details['release_year'] = details['start_date'].dt.year

# Utile car certaines saisons sont plus populaire que d'autre (ex: Automne)
details['month_sin'] = np.sin(2 * np.pi * details['start_date'].dt.month / 12)
details['month_cos'] = np.cos(2 * np.pi * details['start_date'].dt.month / 12)

# Utile car les animés populaires ont les jours de forte audition
details['weekday_sin'] = np.sin(2 * np.pi * details['start_date'].dt.dayofweek / 7)
details['weekday_cos'] = np.cos(2 * np.pi * details['start_date'].dt.dayofweek / 7)

details = details.drop(columns=['year', 'season', 'end_date']) # On drop aussi end_date car sauf cas très spécifiques où la diffusion s'étale sur très longtemps parce que le studio pionce (type Berserk), episodes porte déjà cette info

details = details.drop(['image_url', 'title', 'title_japanese', 'synopsis'], axis=1) # Nécessite du DL

print("1. ", details['members'].corr(details['scored_by'])) # Beaucoup trop proche: Inutile
print("2. ", details['favorites'].corr(details['scored_by'])) # Exploitable, mais peu interprétable

details = details.drop(['rank'], axis=1) # Data Leakage: Le rank d'un nouvel anime est forcément inutilisable
details = details.drop(['scored_by'], axis=1)

###################################################
###################################################
###################################################

###################################################
########### ajout des colonnes utiles #############
###################################################

"""

###################################################
avg_fav = anime_staff_score['total_staff_favorites'] / anime_staff_score['staff_count']
anime_staff_score['staff_quality_log'] = np.log1p(anime_staff_score['total_staff_favorites'] / anime_staff_score['staff_count'])
anime_staff_score['staff_quantity_log'] = np.log1p(anime_staff_score['staff_count'])
###################################################

"""

###################################################
###################################################
###################################################

###################################################
################## One-hot/Bool ###################
###################################################

def one_hot_encode_list_column(df, column_name, prefix=None):
    if prefix is None:
        prefix = f"{column_name}_"

    def parse_list_safe(x):
        if isinstance(x, list): return x
        if pd.isna(x) or x == '': return []
        try:
            # Si c'est une string simple (ex: "TV"), literal_eval va planter,
            # on le met dans une liste pour que le reste du code marche.
            val = ast.literal_eval(x)
            if not isinstance(val, list): # Cas où eval marche mais renvoie un int/dict
                 return [val]
            return val
        except (ValueError, SyntaxError):
            # Cas important : Si la colonne est juste un string "TV" ou "Manga"
            # et pas une liste "['Action']", on retourne le string dans une liste.
            return [x]

    # 1. Parsing
    col_as_list = df[column_name].apply(parse_list_safe)

    # 2. Encodage
    dummies = col_as_list.apply(lambda x: '|'.join(map(str, x))).str.get_dummies(sep='|').astype(int) # int ou bool

    # 3. Prefix
    dummies = dummies.add_prefix(prefix)

    # 4. RETOURNE JUSTE LES DUMMIES (pas le df original)
    return dummies

details_num = details.select_dtypes(include=[np.number]).copy()
details_num['genres'] = details['genres']
details_encoded = one_hot_encode_list_column(details_num, 'genres', prefix='genre_')
details = pd.concat([details, details_encoded], axis=1, join='inner')
details_num = details.select_dtypes(include=[np.number]).copy()
details_num['themes'] = details['themes']
details_encoded = one_hot_encode_list_column(details_num, 'themes', prefix='theme_')
details = pd.concat([details, details_encoded], axis=1, join='inner')
details_num = details.select_dtypes(include=[np.number]).copy()
details_num['demographics'] = details['demographics']
details_encoded = one_hot_encode_list_column(details_num, 'demographics', prefix='demographics_')
details = pd.concat([details, details_encoded], axis=1, join='inner')
details_num = details.select_dtypes(include=[np.number]).copy()
details_num['source'] = details['source']
details_encoded = one_hot_encode_list_column(details_num, 'source', prefix='source_')
details = pd.concat([details, details_encoded], axis=1, join='inner')
details_num = details.select_dtypes(include=[np.number]).copy()
details_num['rating'] = details['rating']
details_encoded = one_hot_encode_list_column(details_num, 'rating', prefix='rating_')
details = pd.concat([details, details_encoded], axis=1, join='inner')

details = details.drop(['genres', 'themes', 'demographics', 'source', 'rating'], axis=1)

print(details_encoded.filter(like='genre_').head())

details['licensors'] = details['licensors'].apply(ast.literal_eval)
details['producers'] = details['producers'].apply(ast.literal_eval)
details['streaming'] = details['streaming'].apply(ast.literal_eval)
details['studios'] = details['studios'].apply(ast.literal_eval)
print(details['producers'].explode().nunique()) # Preuve de haute cardinalité
print(details['licensors'].explode().nunique())
print(details['streaming'].explode().nunique())
print(details['studios'].explode().nunique())
details = details.drop('producers', axis=1) # Inexploitables: Même si on récupérait quelques-un des meilleurs, ce n'est qu'une fraction du total. Et si on splittait par rapport au score, ça introduirait du leakage
best_licensors = details['licensors'].explode().value_counts().head(5).index.tolist()
best_streamer = details['streaming'].explode().value_counts().head(5).index.tolist()
best_studio = details['studios'].explode().value_counts().head(20).index.tolist() # Il existe 2 type de gros studio: Les producteurs en masses, ou ceux de grandes qualité. Il faut les séparer.
print(best_licensors)
print(best_streamer)
print(best_studio)
details['licensors'] = details['licensors'].apply(lambda x: any(item in best_licensors for item in x))
details['streaming'] = details['streaming'].apply(lambda x: any(item in best_streamer for item in x))
temp_studios = details[['mal_id', 'start_date', 'score', 'studios']].copy()
# Si 'studios' est encore une string représentant une liste, il faut l'évaluer
if isinstance(temp_studios['studios'].iloc[0], str):
    temp_studios['studios'] = temp_studios['studios'].apply(lambda x: ast.literal_eval(x) if pd.notna(x) else [])

# On "Explode" pour avoir une ligne par studio par anime
studio_exploded = temp_studios.explode('studios')
studio_exploded = studio_exploded.sort_values('start_date')

# Calcul de la moyenne cumulative décalée (Track Record)
studio_exploded['studio_past_score'] = studio_exploded.groupby('studios')['score']\
    .transform(lambda x: x.shift(1).expanding().mean())

# Fill NA pour les nouveaux studios
studio_exploded['studio_past_score'] = studio_exploded['studio_past_score'].fillna(global_mean_score)

# On regroupe par anime (si un anime a 2 studios, on fait la moyenne de leur réputation)
anime_studio_score = studio_exploded.groupby('mal_id')['studio_past_score'].mean().reset_index()

# Merge final dans details
details = pd.merge(details, anime_studio_score, on='mal_id', how='left')
details['studio_past_score'] = details['studio_past_score'].fillna(global_mean_score)
def filter_top_studios(studio_list):
    if not isinstance(studio_list, list): return []
    return [s for s in studio_list if s in best_studio]
details['studios'] = details['studios'].apply(filter_top_studios) # Voir plus loin pour la définition de cette fonction
studios_encoded = one_hot_encode_list_column(details, 'studios', prefix='studio_')
details = pd.concat([details, studios_encoded], axis=1)
details = details.drop('studios', axis=1)

details = pd.get_dummies(details, columns=['type'], dtype=int)

print(details['status'].value_counts().head(3))
details.drop(details[details['status'] == 'Not yet aired'].index, inplace = True) # Inutilisable: Un anime pas encore sorti ne peut pas être représentatif.

details['status'] = details['status'] == 'Finished Airing'

print(details.head(5))

details['log_members'] = np.log1p(details['members'])
details = details.drop(['members', 'favorites'], axis=1)
details.to_csv('anal-yse.csv', index=False)

#status_dummies = pd.get_dummies(ratings['status'], prefix='status', dtype=bool)

#ratings.drop(columns=["status"], axis=1, inplace=True)
#ratings = pd.concat([ratings, status_dummies], axis=1)
#ratings.drop(columns=["status_unknown"], axis=1, inplace=True) # Inutile: L'info est déjà portée par les autres: status_unknown = 1-\sum(reste)

#print(ratings.head())

###################################################
###################################################
###################################################

###################################################
###################################################
###################################################
#################### Graphes ######################
###################################################
###################################################
###################################################

###############################################
# Preuve que les outliers détruisent la moyenne
###############################################
favs = person_details['favorites'].dropna()
fig, axes = plt.subplots(1, 2, figsize=(15, 5))
sns.histplot(favs, bins=50, ax=axes[0])
axes[0].set_title('Distribution des Favorits')
axes[0].set_xlabel('Nombre de favoris')
axes[0].set_ylabel('Nombre de personnes')
sns.histplot(np.log1p(favs), bins=50, ax=axes[1])
axes[1].set_title('Distribution des Favorits passée au Log)')
axes[1].set_xlabel('Log(Nombre de favoris + 1)')
axes[1].set_ylabel('Nombre de personnes')

plt.tight_layout()
plt.show()
###############################################

###############################################
# Preuve de corrélation logarithmique
###############################################

df_plot = pd.merge(anime_staff_score, details[['mal_id', 'score']],
                   left_on='anime_mal_id', right_on='mal_id')

staff_current = pd.merge(all_staff, person_details[['person_mal_id', 'favorites']], on='person_mal_id', how='left')
staff_current['favorites'] = staff_current['favorites'].fillna(0)
naive_stats = staff_current.groupby('anime_mal_id')['favorites'].sum().reset_index().rename(columns={'favorites': 'total_staff_favorites'})
anime_naive = staff_current.groupby('anime_mal_id')['favorites'].sum().reset_index()
df_plot = pd.merge(df_plot, naive_stats, on='anime_mal_id', how='left')
df_plot = df_plot.dropna(subset=['score'])
df_plot['metric_prof'] = df_plot['total_staff_favorites'] / df_plot['staff_count']
fig, axes = plt.subplots(1, 3, figsize=(20, 6))

sns.regplot(x='metric_prof', y='score', data=df_plot, ax=axes[0],
            line_kws={'color':'red'})
corr1 = df_plot['metric_prof'].corr(df_plot['score'])
axes[0].set_title(f"1. Méthode Suggérée (Linéaire + Fuite)\nCorr: {corr1:.3f}")
axes[0].set_xlabel("Moyenne des Favoris (Total/Count)")

# 2. Graphe Qualité (Log + Sans Fuite)
sns.regplot(x='staff_quality_log', y='score', data=df_plot, ax=axes[1],
            line_kws={'color':'red'})
corr2 = df_plot['staff_quality_log'].corr(df_plot['score'])
axes[1].set_title(f"2. Qualité Staff (Log + Sans Fuite)\n(Basé sur l'historique)\nCorr: {corr2:.3f}")
axes[1].set_xlabel("Log(Score Moyen Historique)")

# 3. Graphe Quantité (Log + Sans Fuite)
sns.regplot(x='staff_quantity_log', y='score', data=df_plot, ax=axes[2],
            line_kws={'color':'red'})
corr3 = df_plot['staff_quantity_log'].corr(df_plot['score'])
axes[2].set_title(f"3. Quantité Staff (Log)\n(Taille de l'équipe)\nCorr: {corr3:.3f}")
axes[2].set_xlabel("Log(Nombre de personnes)")

plt.tight_layout()
plt.show()

# --- OPTIMISATION DU MIX (Alpha) SUR LES DONNÉES SANS FUITE ---
# On garde ça pour justifier le mélange final Qualité/Quantité
z_qual = (df_plot['staff_quality_log'] - df_plot['staff_quality_log'].mean()) / df_plot['staff_quality_log'].std()
z_quant = (df_plot['staff_quantity_log'] - df_plot['staff_quantity_log'].mean()) / df_plot['staff_quantity_log'].std()

target = df_plot['score']
alphas = np.linspace(0, 1, 101)
correlations = []

for alpha in alphas:
    mixed_score = alpha * z_quant + (1 - alpha) * z_qual
    corr = mixed_score.corr(target)
    correlations.append(corr)

best_idx = np.argmax(correlations)
best_alpha = alphas[best_idx]

plt.figure(figsize=(10, 6))
plt.plot(alphas, correlations, label='Corrélation Mixte (Sans Fuite)')
plt.axvline(best_alpha, color='green', linestyle='--', label=f'Optimal : Alpha={best_alpha:.2f}')
plt.title("Preuve de l'utilité des deux métriques")
plt.xlabel("Poids Quantité (0=Qualité seule, 1=Quantité seule)")
plt.ylabel("Corrélation")
plt.legend()
plt.grid(True)
plt.show()

print(f"Mix Optimal (Sans fuite) : {best_alpha*100:.0f}% Quantité / {(1-best_alpha)*100:.0f}% Qualité")

import pandas as pd
import numpy as np
import math
import numpy as np
import ast
import matplotlib.pyplot as plt
import seaborn as sns

#details = pd.read_csv('details.csv')
details['start_date'] = pd.to_datetime(details['start_date'], errors='coerce')
details = details.sort_values('start_date')
global_mean_score = details['score'].mean()
print(global_mean_score)

import numpy as np
from sklearn.linear_model import ElasticNet
from sklearn.tree import DecisionTreeRegressor
from sklearn.base import clone
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.model_selection import train_test_split

# --- FONCTION DE PRÉDICTION ---
def predict_composite(X, router, local_models, global_model_fallback):
    # 1. Trouver les feuilles
    leaf_ids = router.apply(X)
    predictions = np.zeros(len(X))
    unique_leaves = np.unique(leaf_ids)

    for leaf in unique_leaves:
        mask = (leaf_ids == leaf)

        if leaf in local_models:
            # Cas normal : on a un expert pour cette feuille
            predictions[mask] = local_models[leaf].predict(X[mask])
        else:
            # Cas rare : une feuille du test n'était pas dans le train
            # On utilise le modèle global comme roue de secours
            predictions[mask] = global_model_fallback.predict(X[mask])

    return predictions

# --- PRÉPARATION DES DONNÉES ---
# (Je suppose que 'details' est déjà chargé)
details = details.fillna(details.mean(numeric_only=True))
y = details['log_members']
columns_to_drop = [
    'log_members',      # La cible (évidemment)
    'members',          # La cible brute (au cas où elle traîne)
    'favorites',        # Corrélation trop forte (souvent 0.8+) et dépend du succès
    'popularity',       # <--- LE GRAND COUPABLE (C'est juste un tri de members)
    'rank',             # Dépend du score, qui dépend des votes
    'scored_by',        # Dépend directement du nombre de membres
    'score',            # Leak du futur (on ne connait pas le score avant la diffusion)
    'controversy_score',# Leak du futur (dépend des votes)
    'start_date',       # Type datetime, pas géré par ElasticNet (tu as déjà extrait month/year/weekday)
    'mal_id',           # Juste un ID, bruit pur
    'start_date'
]

# On ne drop que ce qui existe dans le dataframe pour éviter les erreurs
existing_drops = [col for col in columns_to_drop if col in details.columns]
X = details.drop(existing_drops, axis=1)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 1. ENTRAÎNEMENT GLOBAL
global_model = ElasticNet(alpha=1.0, l1_ratio=0.5)
global_model.fit(X_train, y_train)

# 2. ROUTAGE (DT)
router = DecisionTreeRegressor(max_depth=4, min_samples_leaf=50)
router.fit(X_train, y_train)
leaf_ids = router.apply(X_train)
unique_leaves = np.unique(leaf_ids)

# 3. SPÉCIALISATION (BOUCLE D'ENTRAÎNEMENT)
local_models = {}

print(f"Entraînement de {len(unique_leaves)} experts locaux...")

for leaf in unique_leaves:
    mask = (leaf_ids == leaf)
    X_leaf, y_leaf = X_train[mask], y_train[mask]

    # Clonage et Config
    leaf_model = clone(global_model)
    leaf_model.warm_start = True

    # Injection du savoir global
    leaf_model.coef_ = global_model.coef_.copy()
    leaf_model.intercept_ = global_model.intercept_

    # Fine-tuning (Une seule fois suffit avec le bon nombre d'itérations)
    # On met un max_iter raisonnable pour éviter l'overfitting local
    leaf_model.max_iter = 200

    leaf_model.fit(X_leaf, y_leaf)
    local_models[leaf] = leaf_model

# --- 4. ÉVALUATION FINALE (HORS DE LA BOUCLE) ---
print("Calcul des performances sur le jeu de test...")

# On prédit sur X_TEST (pas X_leaf)
y_pred_test = predict_composite(X_test, router, local_models, global_model)

# On compare avec Y_TEST
r2 = r2_score(y_test, y_pred_test)
rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))
mae = mean_absolute_error(y_test, y_pred_test)

print("-" * 30)
print(f"PERFORMANCE GLOBALE SUR LE TEST")
print("-" * 30)
print(f"R² (Variance expliquée) : {r2:.4f}")
print(f"RMSE (Erreur moyenne)   : {rmse:.4f}")
print(f"MAE (Erreur absolue)    : {mae:.4f}")
print("-" * 30)


# --- 5. COMPARAISON AVEC LE MODÈLE GLOBAL SEUL ---
print("\n" + "="*30)
print("COMPARAISON BASELINE")
print("="*30)

# Prédiction du modèle global seul sur le test
y_pred_global = global_model.predict(X_test)

r2_glob = r2_score(y_test, y_pred_global)
rmse_glob = np.sqrt(mean_squared_error(y_test, y_pred_global))

print(f"Modèle Global R²    : {r2_glob:.4f}")
print(f"Modèle Composite R² : {r2:.4f}")
print(f"Gain de R²          : {r2 - r2_glob:.4f}")
print("-" * 30)
print(f"Modèle Global RMSE    : {rmse_glob:.4f}")
print(f"Modèle Composite RMSE : {rmse:.4f}")
print(f"Gain de RMSE          : {rmse_glob - rmse:.4f}") # On veut que le RMSE baisse

X.head()

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.tree import plot_tree

# Configuration Style Pro (Good Quality requise)
plt.style.use('seaborn-v0_8-whitegrid')
plt.rcParams.update({'font.size': 11, 'figure.dpi': 300})

# ==============================================================================
# FIGURE 1 : VISUALISATION DU "ROUTEUR" (La segmentation)
# Cette figure illustre la phase 1 de ta méthodologie : diviser pour mieux régner
# ==============================================================================
plt.figure(figsize=(12, 6))
plot_tree(router,
          max_depth=2,              # On n'affiche que le haut pour que ce soit lisible
          feature_names=X.columns,  # Noms des variables
          filled=True,              # Couleurs
          rounded=True,
          fontsize=10,
          proportion=True)          # Affiche % de données, pas juste le nombre

plt.title("Phase 1: Segmentation Strategy (Decision Tree Router)", fontsize=14, fontweight='bold')
plt.tight_layout()
plt.savefig("methodology_tree_router.png")
plt.show()
print("Figure 'methodology_tree_router.png' générée.")

# ==============================================================================
# FIGURE 2 : COMPARAISON GLOBAL vs COMPOSITE (La justification)
# Cette figure prouve l'efficacité de ta méthodologie
# ==============================================================================
plt.figure(figsize=(10, 5))

# Création d'un dataframe pour faciliter le plot
df_res = pd.DataFrame({
    'Actual Score': y_test,
    'Global Model (ElasticNet)': y_pred_global,
    'Hybrid Model (Tree + ElasticNet)': y_pred_test
})

# Sous-plot 1 : Modèle Global
plt.subplot(1, 2, 1)
sns.scatterplot(x='Actual Score', y='Global Model (ElasticNet)', data=df_res,
                alpha=0.3, color='gray', s=15)
# Ligne parfaite idéale
plt.plot([1, 10], [1, 10], 'r--', lw=2, label='Perfect Prediction')
plt.title(f"Baseline: Global Model\nRMSE = {rmse_glob:.3f}", fontsize=11)
plt.xlabel("Actual Anime Score")
plt.ylabel("Predicted Score")
plt.legend()
plt.axis('equal') # Mêmes échelles x et y

# Sous-plot 2 : Ton Modèle Hybride
plt.subplot(1, 2, 2)
sns.scatterplot(x='Actual Score', y='Hybrid Model (Tree + ElasticNet)', data=df_res,
                alpha=0.3, color='#007acc', s=15) # Bleu "Pro"
plt.plot([1, 10], [1, 10], 'r--', lw=2, label='Perfect Prediction')
plt.title(f"Proposed: Hybrid Model\nRMSE = {rmse:.3f} (Improved)", fontsize=11, fontweight='bold')
plt.xlabel("Actual Anime Score")
plt.ylabel("Predicted Score")
plt.legend()
plt.axis('equal')

plt.suptitle("Methodology Validation: Prediction Accuracy Comparison", fontsize=14, fontweight='bold', y=1.02)
plt.tight_layout()
plt.savefig("methodology_performance_comparison.png")
plt.show()
print("Figure 'methodology_performance_comparison.png' générée.")

import matplotlib.pyplot as plt
import seaborn as sns

# Config style
plt.style.use('seaborn-v0_8-whitegrid')
plt.rcParams.update({'font.size': 12, 'figure.dpi': 300})

# Calcul des Résidus (Erreur = Réalité - Prédiction)
# C'est littéralement ce que la fonction coût mesure
residuals_global = y_test - y_pred_global
residuals_hybrid = y_test - y_pred_test

plt.figure(figsize=(10, 6))

# 1. Plot du Modèle Global (En gris/rouge)
sns.kdeplot(residuals_global, fill=True, color='red', alpha=0.3,
            label=f'Global Model (RMSE={rmse_glob:.2f})')

# 2. Plot du Modèle Hybride (En bleu/vert)
sns.kdeplot(residuals_hybrid, fill=True, color='green', alpha=0.4,
            label=f'Hybrid Model (RMSE={rmse:.2f})')

# Ligne zéro (Erreur parfaite)
plt.axvline(0, color='black', linestyle='--', linewidth=1)

plt.title("Comparison of Cost Function (Residual Errors Distribution)", fontweight='bold')
plt.xlabel("Prediction Error (True Score - Predicted Score)")
plt.ylabel("Density")
plt.xlim(-4, 4) # On zoome sur les erreurs raisonnables
plt.legend()

plt.tight_layout()
plt.savefig("cost_function_comparison.png")
plt.show()
print("Image 'cost_function_comparison.png' générée.")

details['score']/len(details['score'])

# --- PRÉPARATION SPÉCIFIQUE CIBLE 1 (SCORE) ---
# On recharge X pour être propre
X_score = details.copy()
y_score = X_score['score'] # La cible change

# Liste des colonnes interdites pour le Score
# On enlève ce qui est une conséquence directe du score ou du futur
drops_score = [
    'score',            # Target
    'rank',             # Dérivé direct du score
    'scored_by',        # Lié au volume de votes
    'favorites',        # Corrélation forte post-diffusion
    'mal_id',
    'start_date'
    # Note : On garde 'log_members' ou 'members' si on considère
    # que la hype (marketing) existe avant la note critique.
    # Si tu veux être puriste comme le rapport : on drop aussi 'log_members'.
]
# Pour respecter le rapport à la lettre (Isolation totale) :
drops_score.append('log_members')
drops_score.append('members')
drops_score.append('popularity')

existing_drops_score = [col for col in drops_score if col in X_score.columns]
X_score = X_score.drop(existing_drops_score, axis=1)

# Split
X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(X_score, y_score, test_size=0.2, random_state=42)

# --- ENTRAÎNEMENT (Même logique mais avec ElasticNetCV pour optimiser alpha) ---
from sklearn.linear_model import ElasticNetCV

print("\n--- ENTRAÎNEMENT CIBLE 1 : SCORE ---")

# 1. Global (Avec CV intégré pour trouver le meilleur alpha automatiquement)
# Cela améliore souvent grandement la précision
global_model_s = ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, 1], cv=5, random_state=42)
global_model_s.fit(X_train_s, y_train_s)
print(f"Alpha optimal global : {global_model_s.alpha_}")

# 2. Routeur
router_s = DecisionTreeRegressor(max_depth=4, min_samples_leaf=50)
router_s.fit(X_train_s, y_train_s)
leaf_ids_s = router_s.apply(X_train_s)
unique_leaves_s = np.unique(leaf_ids_s)

# 3. Spécialisation
local_models_s = {}

for leaf in unique_leaves_s:
    mask = (leaf_ids_s == leaf)
    X_leaf, y_leaf = X_train_s[mask], y_train_s[mask]

    # On utilise ElasticNet standard ici pour la rapidité, configuré avec l'alpha global
    # ou on refait une CV locale si on a le temps de calcul
    leaf_model = ElasticNet(alpha=global_model_s.alpha_, l1_ratio=global_model_s.l1_ratio_)

    # Transfert learning (Initialisation)
    leaf_model.coef_ = global_model_s.coef_.copy()
    leaf_model.intercept_ = global_model_s.intercept_

    leaf_model.fit(X_leaf, y_leaf)
    local_models_s[leaf] = leaf_model

# 4. Prédiction et Métriques
y_pred_test_s = predict_composite(X_test_s, router_s, local_models_s, global_model_s)

rmse_s = np.sqrt(mean_squared_error(y_test_s, y_pred_test_s))
print(f"RMSE Score (Model Tree) : {rmse_s:.4f}")

from sklearn.linear_model import LogisticRegressionCV
from sklearn.metrics import accuracy_score, classification_report

# --- PRÉPARATION ---
# Supposons que tu as déjà calculé ta colonne cible 'is_recommended' (0 ou 1)
# via le Z-score dans ton dataframe 'details' ou un dataframe joint users/animes.
# Si c'est au niveau "Anime global" (ex: est-ce un anime universellement recommandé ?),
# on garde une ligne par anime.

# Cible binaire
# Ex: on crée une dummy target pour l'exemple si elle n'existe pas
if 'is_recommended' not in details.columns:
    # Simulation basée sur le score pour l'exemple (à remplacer par ton calcul Z-score)
    details['is_recommended'] = (details['score'] > 7.5).astype(int)

y_rec = details['is_recommended']
X_rec = X.copy() # On reprend les features nettoyées (sans leaks)

X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X_rec, y_rec, test_size=0.2, random_state=42, stratify=y_rec)

print("\n--- ENTRAÎNEMENT CIBLE 3 : RECOMMANDATION (LOGISTIQUE) ---")

# 1. Modèle Global Logistique (avec ElasticNet)
# Solver 'saga' est requis pour ElasticNet en logistique
global_clf = LogisticRegressionCV(
    cv=5,
    penalty='elasticnet',
    solver='saga',
    l1_ratios=[0.5], # On peut en tester plusieurs
    max_iter=5000,
    random_state=42
)
global_clf.fit(X_train_r, y_train_r)

# 2. Routeur (On garde un Arbre de Régression ou Classification, les deux marchent pour router)
# Ici un Classifier pour mieux séparer les classes 0 et 1
from sklearn.tree import DecisionTreeClassifier
router_clf = DecisionTreeClassifier(max_depth=4, min_samples_leaf=50)
router_clf.fit(X_train_r, y_train_r)
leaf_ids_r = router_clf.apply(X_train_r)
unique_leaves_r = np.unique(leaf_ids_r)

# 3. Spécialisation
local_clfs = {}

for leaf in unique_leaves_r:
    mask = (leaf_ids_r == leaf)
    X_leaf, y_leaf = X_train_r[mask], y_train_r[mask]

    # Attention : Si une feuille ne contient qu'une seule classe (que des 0 ou que des 1),
    # La régression logistique va planter ou warning.
    if len(np.unique(y_leaf)) < 2:
        # On stocke la classe majoritaire comme prédiction constante
        local_clfs[leaf] = int(y_leaf.iloc[0])
    else:
        # On entraîne un modèle local
        leaf_clf = LogisticRegression(
            penalty='elasticnet',
            solver='saga',
            l1_ratio=0.5,
            C=global_clf.C_[0], # On reprend la force de régularisation optimale globale
            max_iter=2000
        )
        # Warm start manuel (coeff init)
        leaf_clf.coef_ = global_clf.coef_.copy()
        leaf_clf.intercept_ = global_clf.intercept_

        leaf_clf.fit(X_leaf, y_leaf)
        local_clfs[leaf] = leaf_clf

# Fonction de prédiction adaptée pour la classification
def predict_composite_class(X, router, local_models, global_fallback):
    leaf_ids = router.apply(X)
    predictions = np.zeros(len(X))
    unique_leaves = np.unique(leaf_ids)

    for leaf in unique_leaves:
        mask = (leaf_ids == leaf)
        if leaf in local_models:
            model = local_models[leaf]
            if isinstance(model, int): # Cas classe constante
                predictions[mask] = model
            else:
                predictions[mask] = model.predict(X[mask])
        else:
            predictions[mask] = global_fallback.predict(X[mask])
    return predictions

y_pred_r = predict_composite_class(X_test_r, router_clf, local_clfs, global_clf)

print("Accuracy Recommandation :", accuracy_score(y_test_r, y_pred_r))